- 3個action 0: 向左, 1: 向前, 2: 向右
- CNN policy model


---
- 狀態
  - LIST (11)
    - 蛇頭左、前、右有無障礙物
    - 蛇頭和食物之間的向量(x, y)
    - 蛇的方向
    - 壽命(剩餘多少找尋食物的時間)
    - 蛇的長度
  - IMAGE (12, 12, 1)
  (幫助預防蛇被自己包圍)
    - 蛇在遊戲中的位置
---
- REWARD
    | 項目      | 點數     |
    | -----------   | -----------: |
    | 撞到牆、自己   |-10        |
    | 餓死          | -20        |
    | 吃到食物   | +20        |
    | 靠近食物   | +1       |
    | 遠離食物   | -1       |
---
- POLICY網路
  - IMAGE經過卷積層、FLATTEN
  - LIST和IMAGE concat
  - 送入全連接層
  - 輸出 Q table
---
- 訓練
  - Double DQN
    | <!-- -->    | <!-- -->    |
    |:-------------|-------------:|
    | Loss fuction | MSELoss |
    | Optimizer | Adam |

  - 固定每個eposide的亂數種子
  - ε-greedy policy
  - 經驗回放
  - 每個eposide結束會驗證model成效，將best儲存
- 比較
  - CNN
    - 3point
    - 9point
    - 32point
  - FCN

---
---
- 之前試過的方法
  - Monte-carlo policy-gradient
  蒙特卡羅方法（Monte Carlo methods）在強化學習中的應用主要體現在透過模擬來估計策略的價值。對於像貪吃蛇這樣的問題，蒙特卡羅方法可以用來估計每個狀態的價值，並透過多次完整的遊戲模擬來改進策略。不過，與DQN及其變種相比，蒙特卡羅方法也有一些局限性，特別是在以下方面：

  - 採樣效率：蒙特卡羅方法依賴完整的回合模擬，可能需要大量的模擬回合才能收斂，特別是在具有長決策序列的問題上。
  - 探索與利用：蒙特卡羅方法通常不如基於Q-learning的策略靈活，在處理探索與利用的平衡時可能表現較差。
  - 連續狀態空間：在處理較複雜的狀態空間時，蒙特卡羅方法可能不如基於函數逼近的DQN方法高效。
  - 對於像貪吃蛇這樣的遊戲，DQN及其變種通常更為合適，因為它們能夠更有效率地學習狀態-動作價值函數，並且能夠很好地處理複雜的狀態空間。
